{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4mgnq/Gx0+4MuRGZ6hT/p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiz-oliveir/LSTM/blob/main/LSTM_VAE_com_ajustes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Configure matplotlib backend based on environment\n",
        "if 'ipykernel' in sys.modules:\n",
        "    # Running in Jupyter/IPython\n",
        "    try:\n",
        "        import IPython\n",
        "        ipython = IPython.get_ipython()\n",
        "        ipython.run_line_magic('matplotlib', 'inline')\n",
        "    except Exception:\n",
        "        os.environ['MPLBACKEND'] = 'TkAgg'\n",
        "else:\n",
        "    # Running as script\n",
        "    os.environ['MPLBACKEND'] = 'TkAgg'\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(0)\n",
        "import glob\n",
        "from keras_tuner import RandomSearch\n",
        "import sys\n",
        "import keras_tuner as kt\n",
        "import pickle\n",
        "import datetime\n",
        "\n",
        "# Configuração otimizada para GPU NVIDIA/CUDA\n",
        "def setup_gpu():\n",
        "    print(\"\\nVerificando configuração GPU:\")\n",
        "    print(\"TensorFlow versão:\", tf.__version__)\n",
        "    print(\"CUDA disponível:\", tf.test.is_built_with_cuda())\n",
        "    print(\"GPU disponível para TensorFlow:\", tf.test.is_gpu_available())\n",
        "\n",
        "    try:\n",
        "        # Listar GPUs disponíveis\n",
        "        gpus = tf.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            print(\"\\nGPUs disponíveis:\", len(gpus))\n",
        "            for gpu in gpus:\n",
        "                print(\" -\", gpu.name)\n",
        "\n",
        "            # Permitir crescimento de memória dinâmico\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "            # Configurar para formato de dados mixed precision\n",
        "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "            tf.keras.mixed_precision.set_global_policy(policy)\n",
        "\n",
        "            print(\"\\nGPU configurada com sucesso!\")\n",
        "            print(\"Usando mixed precision:\", policy.name)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"\\nNenhuma GPU encontrada. Usando CPU.\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\nErro ao configurar GPU:\", str(e))\n",
        "        print(\"Usando CPU como fallback.\")\n",
        "        return False\n",
        "\n",
        "# Configurar GPU no início do script\n",
        "using_gpu = setup_gpu()\n",
        "\n",
        "from tensorflow import keras, data\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras import layers, regularizers, activations, optimizers\n",
        "from tensorflow.keras import backend as K\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dataset_name = \"bearing_dataset\"  # Apenas para referência\n",
        "#train_ratio = 0.75\n",
        "row_mark = 740\n",
        "batch_size = 128\n",
        "time_step = 1\n",
        "x_dim = 4\n",
        "lstm_h_dim = 8\n",
        "z_dim = 4\n",
        "epoch_num = 100\n",
        "threshold = None\n",
        "\n",
        "mode = 'train'\n",
        "model_dir = \"./lstm_vae_model/\"\n",
        "image_dir = \"./lstm_vae_images/\"\n",
        "results_dir = 'C:/Users/Augusto-PC/Documents/GitHub/LSTM/Resumo resultados/'\n",
        "\n",
        "# Criar diretórios se não existirem\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Ler o diretório de dados do arquivo data_path.txt\n",
        "try:\n",
        "    with open('data_path.txt', 'r') as f:\n",
        "        data_dir = f.read().strip()\n",
        "    print(f\"Usando diretório de dados: {data_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao ler data_path.txt: {str(e)}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Criar diretórios necessários\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Parâmetros de ativação\n",
        "lstm_activation = 'softplus'  # Pode mudar para 'tanh', 'relu', etc\n",
        "sigma_activation = 'tanh'     # Ativação para sigma_x\n",
        "\n",
        "def split_normalize_data(all_df):\n",
        "    #row_mark = int(all_df.shape[0] * train_ratio)\n",
        "    train_df = all_df[:row_mark]\n",
        "    test_df = all_df[row_mark:]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(np.array(all_df)[:, 1:])\n",
        "    train_scaled = scaler.transform(np.array(train_df)[:, 1:])\n",
        "    test_scaled = scaler.transform(np.array(test_df)[:, 1:])\n",
        "    return train_scaled, test_scaled\n",
        "\n",
        "def reshape(da):\n",
        "    return da.reshape(da.shape[0], time_step, da.shape[1]).astype(\"float32\")\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Camada de amostragem usando o truque de reparametrização\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mu, logvar = inputs\n",
        "        batch = tf.shape(mu)[0]\n",
        "        dim = tf.shape(mu)[1]\n",
        "\n",
        "        # Garantir que todos os tensores estejam no mesmo dtype\n",
        "        dtype = mu.dtype\n",
        "        epsilon = tf.random.normal(shape=(batch, dim), dtype=dtype)\n",
        "\n",
        "        # Calcular usando o mesmo dtype\n",
        "        return mu + tf.cast(tf.exp(0.5 * logvar), dtype) * epsilon\n",
        "\n",
        "class Encoder(layers.Layer):\n",
        "    def __init__(self, time_step, x_dim, lstm_h_dim, z_dim, name='encoder', activation=lstm_activation, **kwargs):\n",
        "        super(Encoder, self).__init__(name=name, **kwargs)\n",
        "        self.time_step = time_step\n",
        "        self.x_dim = x_dim\n",
        "        self.lstm_h_dim = lstm_h_dim\n",
        "        self.z_dim = z_dim\n",
        "        self.activation = activation\n",
        "\n",
        "        # Camadas do encoder\n",
        "        self.lstm = layers.LSTM(\n",
        "            lstm_h_dim,\n",
        "            activation=activation,\n",
        "            return_sequences=True,\n",
        "            name='encoder_lstm'\n",
        "        )\n",
        "\n",
        "        self.flatten = layers.Flatten(name='encoder_flatten')\n",
        "        self.dense = layers.Dense(lstm_h_dim, activation=activation, name='encoder_dense')\n",
        "\n",
        "        # Camadas para média e log variância\n",
        "        self.z_mean = layers.Dense(z_dim, name='z_mean')\n",
        "        self.z_log_var = layers.Dense(z_dim, name='z_log_var')\n",
        "\n",
        "        # Camada de amostragem\n",
        "        self.sampling = Sampling()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Passar pela LSTM\n",
        "        x = self.lstm(inputs)\n",
        "\n",
        "        # Flatten e dense\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "\n",
        "        # Calcular média e log variância\n",
        "        z_mean = self.z_mean(x)\n",
        "        z_log_var = self.z_log_var(x)\n",
        "\n",
        "        return z_mean, z_log_var\n",
        "\n",
        "    def sampling(self, inputs):\n",
        "        \"\"\"Amostra do espaço latente\"\"\"\n",
        "        z_mean, z_log_var = inputs\n",
        "        return self.sampling([z_mean, z_log_var])\n",
        "\n",
        "class Decoder(layers.Layer):\n",
        "    def __init__(self, time_step, x_dim, lstm_h_dim, z_dim, name='decoder', activation=lstm_activation, sigma_activation=sigma_activation, **kwargs):\n",
        "        super(Decoder, self).__init__(name=name, **kwargs)\n",
        "\n",
        "        self.z_inputs = layers.RepeatVector(time_step, name='repeat_vector')\n",
        "        self.decoder_lstm_hidden = layers.LSTM(\n",
        "            lstm_h_dim,\n",
        "            activation=activation,\n",
        "            return_sequences=True,\n",
        "            name='decoder_lstm'\n",
        "        )\n",
        "        self.x_mean = layers.Dense(x_dim, name='x_mean')\n",
        "        self.x_sigma = layers.Dense(\n",
        "            x_dim,\n",
        "            name='x_sigma',\n",
        "            activation=sigma_activation  # Usar parâmetro\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z = self.z_inputs(inputs)\n",
        "        hidden = self.decoder_lstm_hidden(z)\n",
        "        mu_x = self.x_mean(hidden)\n",
        "        sigma_x = self.x_sigma(hidden)\n",
        "        return mu_x, sigma_x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Decoder, self).get_config()\n",
        "        config.update({\n",
        "            'name': self.name\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class LSTMVAE(keras.Model):\n",
        "    def __init__(self, time_step, x_dim, lstm_h_dim, z_dim, name='lstm_vae', **kwargs):\n",
        "        super(LSTMVAE, self).__init__(name=name, **kwargs)\n",
        "        self.encoder = Encoder(time_step, x_dim, lstm_h_dim, z_dim)\n",
        "        self.decoder = Decoder(time_step, x_dim, lstm_h_dim, z_dim)\n",
        "        self.time_step = time_step\n",
        "        self.x_dim = x_dim\n",
        "        self.z_dim = z_dim\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "        self.sampling = Sampling()\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = self.encoder(inputs)\n",
        "        z = self.sampling([z_mean, z_log_var])\n",
        "        reconstruction = self.decoder(z)\n",
        "        return reconstruction\n",
        "\n",
        "    def predict(self, inputs, batch_size=None, verbose=0, steps=None):\n",
        "        \"\"\"Retorna z_mean, z_log_var e log_px para os inputs\"\"\"\n",
        "        # Processar os inputs diretamente\n",
        "        z_mean, z_log_var = self.encoder(inputs)\n",
        "        z = self.sampling([z_mean, z_log_var])\n",
        "        reconstruction = self.decoder(z)\n",
        "\n",
        "        # Calcular log p(x|z)\n",
        "        log_px = -tf.reduce_sum(\n",
        "            keras.losses.mse(inputs, reconstruction),\n",
        "            axis=[1, 2]\n",
        "        )\n",
        "\n",
        "        return z_mean, z_log_var, log_px\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var = self.encoder(data)\n",
        "            z = self.sampling([z_mean, z_log_var])\n",
        "            reconstruction = self.decoder(z)\n",
        "\n",
        "            # Calcular reconstruction loss\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.mse(data, reconstruction),\n",
        "                    axis=[1, 2]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Calcular KL loss\n",
        "            kl_loss = -0.5 * tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var),\n",
        "                    axis=1\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Loss total\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        # Calcular gradientes e atualizar pesos\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        # Atualizar métricas\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        z_mean, z_log_var = self.encoder(data)\n",
        "        z = self.sampling([z_mean, z_log_var])\n",
        "        reconstruction = self.decoder(z)\n",
        "\n",
        "        # Calcular reconstruction loss\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "            tf.reduce_sum(\n",
        "                keras.losses.mse(data, reconstruction),\n",
        "                axis=[1, 2]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Calcular KL loss\n",
        "        kl_loss = -0.5 * tf.reduce_mean(\n",
        "            tf.reduce_sum(\n",
        "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var),\n",
        "                axis=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Loss total\n",
        "        total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        # Atualizar métricas\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "def prepare_training_data(all_df, batch_size=128):\n",
        "    print(\"Pre-processing data...\")\n",
        "\n",
        "    # Normalizar os dados\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(all_df)\n",
        "\n",
        "    # Reshape para formato LSTM (amostras, time_steps, features)\n",
        "    n_samples = scaled_data.shape[0] - time_step\n",
        "    X = np.zeros((n_samples, time_step, scaled_data.shape[1]))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        X[i] = scaled_data[i:i + time_step]\n",
        "\n",
        "    # Dividir em treino e teste\n",
        "    train_size = int(0.8 * n_samples)\n",
        "    X_train = X[:train_size]\n",
        "    X_test = X[train_size:]\n",
        "\n",
        "    # Converter para float16\n",
        "    X_train = tf.cast(X_train, tf.float16)\n",
        "    X_test = tf.cast(X_test, tf.float16)\n",
        "\n",
        "    # Criar datasets\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
        "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "    test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\n",
        "    test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    print(f\"Shape dos dados de treino: {X_train.shape}\")\n",
        "    print(f\"Shape dos dados de teste: {X_test.shape}\")\n",
        "\n",
        "    return train_dataset, test_dataset, scaler\n",
        "\n",
        "def hyperparameter_tuning(train_dataset):\n",
        "    \"\"\"Otimização de hiperparâmetros usando keras-tuner\"\"\"\n",
        "\n",
        "    class VAEHyperModel(kt.HyperModel):\n",
        "        def __init__(self, time_step, input_dim):\n",
        "            super().__init__()\n",
        "            self.time_step = time_step\n",
        "            self.input_dim = input_dim\n",
        "\n",
        "        def build(self, hp):\n",
        "            # Hiperparâmetros\n",
        "            hp_lstm_dim = hp.Int('lstm_dim', min_value=8, max_value=32, step=4)\n",
        "            hp_z_dim = hp.Int('z_dim', min_value=2, max_value=16, step=2)\n",
        "            hp_learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "\n",
        "            # Configurar política de mixed precision\n",
        "            tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "            # Criar modelo\n",
        "            model = LSTMVAE(\n",
        "                time_step=self.time_step,\n",
        "                x_dim=self.input_dim,\n",
        "                lstm_h_dim=hp_lstm_dim,\n",
        "                z_dim=hp_z_dim\n",
        "            )\n",
        "\n",
        "            # Compilar modelo com loss scaling para mixed precision\n",
        "            optimizer = keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
        "            optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
        "            model.compile(optimizer=optimizer)\n",
        "\n",
        "            # Construir modelo explicitamente\n",
        "            dummy_data = tf.zeros((1, self.time_step, self.input_dim), dtype=tf.float16)\n",
        "            _ = model(dummy_data)\n",
        "\n",
        "            return model\n",
        "\n",
        "    # Criar diretório para o tuner se não existir\n",
        "    tuner_dir = os.path.join(os.path.dirname(model_dir), 'keras_tuner')\n",
        "    os.makedirs(tuner_dir, exist_ok=True)\n",
        "\n",
        "    # Obter as dimensões do dataset\n",
        "    for batch in train_dataset.take(1):\n",
        "        input_shape = batch.shape\n",
        "        input_dim = input_shape[-1]\n",
        "        break\n",
        "\n",
        "    # Calcular tamanho do dataset\n",
        "    dataset_size = sum(1 for _ in train_dataset)\n",
        "    val_size = int(0.2 * dataset_size)\n",
        "    train_size = dataset_size - val_size\n",
        "\n",
        "    # Criar datasets de treino e validação\n",
        "    train_data = train_dataset.take(train_size)\n",
        "    val_data = train_dataset.skip(train_size)\n",
        "\n",
        "    print(f\"Tamanho do dataset de treino: {train_size}\")\n",
        "    print(f\"Tamanho do dataset de validação: {val_size}\")\n",
        "\n",
        "    # Configurar tuner\n",
        "    tuner = kt.RandomSearch(\n",
        "        hypermodel=VAEHyperModel(\n",
        "            time_step=time_step,\n",
        "            input_dim=input_dim\n",
        "        ),\n",
        "        objective='val_loss',\n",
        "        max_trials=5,\n",
        "        executions_per_trial=1,\n",
        "        directory=tuner_dir,\n",
        "        project_name='lstm_vae'\n",
        "    )\n",
        "\n",
        "    # Configurar callbacks\n",
        "    callbacks = [\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Buscar melhores hiperparâmetros\n",
        "    tuner.search(\n",
        "        train_data,\n",
        "        validation_data=val_data,\n",
        "        epochs=50,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Obter e mostrar melhores hiperparâmetros\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    print(\"\\nMelhores hiperparâmetros encontrados:\")\n",
        "    print(f\"LSTM dim: {best_hps.get('lstm_dim')}\")\n",
        "    print(f\"Z dim: {best_hps.get('z_dim')}\")\n",
        "    print(f\"Learning rate: {best_hps.get('learning_rate')}\")\n",
        "\n",
        "    # Retornar melhor modelo\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "    return best_model\n",
        "\n",
        "def generate_sample_data():\n",
        "    \"\"\"Generate sample data if no Excel files are found.\"\"\"\n",
        "    print(\"\\nGenerating sample data for testing...\")\n",
        "\n",
        "    # Create sample timestamps\n",
        "    dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='H')\n",
        "    n_samples = len(dates)\n",
        "\n",
        "    # Generate synthetic data\n",
        "    np.random.seed(42)\n",
        "    data = {\n",
        "        'timestamp': dates,\n",
        "        'nivel_agua': np.random.normal(100, 10, n_samples),  # Normal distribution\n",
        "        'vazao': np.abs(np.random.normal(50, 5, n_samples)),  # Positive values\n",
        "        'temperatura': np.random.normal(25, 3, n_samples),  # Normal distribution\n",
        "    }\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Add some anomalies\n",
        "    anomaly_idx = np.random.choice(n_samples, size=int(n_samples * 0.05), replace=False)\n",
        "    df.loc[anomaly_idx, 'nivel_agua'] *= 1.5\n",
        "    df.loc[anomaly_idx, 'vazao'] *= 2\n",
        "    df.loc[anomaly_idx, 'temperatura'] += 10\n",
        "\n",
        "    # Save to Excel file\n",
        "    sample_file = os.path.join(data_dir, 'sample_data.xlsx')\n",
        "    print(f\"Saving sample data to {sample_file}\")\n",
        "    df.to_excel(sample_file, index=False)\n",
        "    return df\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    print(\"\\nCarregando e preparando dados...\")\n",
        "    print(f\"Diretório de dados: {data_dir}\")\n",
        "\n",
        "    dfs = []\n",
        "\n",
        "    # Get list of Excel files (ignorando arquivos temporários que começam com ~$)\n",
        "    excel_files = [f for f in glob.glob(os.path.join(data_dir, \"*.xlsx\")) if not os.path.basename(f).startswith('~$')]\n",
        "\n",
        "    if not excel_files:\n",
        "        print(f\"\\nERRO: Nenhum arquivo Excel encontrado em {data_dir}\")\n",
        "        print(\"\\nPor favor, verifique:\")\n",
        "        print(\"1. Se o diretório está correto\")\n",
        "        print(\"2. Se os arquivos Excel (.xlsx) foram carregados\")\n",
        "        print(\"3. Se os arquivos não estão em uma subpasta\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"\\nEncontrados {len(excel_files)} arquivos Excel:\")\n",
        "    for f in excel_files[:5]:\n",
        "        print(f\"  - {os.path.basename(f)}\")\n",
        "    if len(excel_files) > 5:\n",
        "        print(f\"  ... e mais {len(excel_files)-5} arquivos\")\n",
        "\n",
        "    # Define required columns\n",
        "    required_cols = {\n",
        "        'Data': ['Data', 'DATA', 'data'],\n",
        "        'temperatura': ['TEMPERATURA MAXIMA, DIARIA(Â°C)', 'TEMPERATURA MAXIMA, DIARIA(°C)', 'TEMPERATURA MAXIMA DIARIA']\n",
        "    }\n",
        "\n",
        "    # Load and combine data from all files\n",
        "    for file in excel_files:\n",
        "        print(f\"\\nCarregando {os.path.basename(file)}...\")\n",
        "        try:\n",
        "            # Ler o Excel com parse_dates e dayfirst=True\n",
        "            df = pd.read_excel(\n",
        "                file,\n",
        "                parse_dates=['Data'],\n",
        "                date_parser=lambda x: pd.to_datetime(x, dayfirst=True)\n",
        "            )\n",
        "            print(f\"Colunas encontradas: {list(df.columns)}\")\n",
        "\n",
        "            # Map actual columns to required columns\n",
        "            column_mapping_found = {}\n",
        "            for required_col, possible_names in required_cols.items():\n",
        "                found_col = next((col for col in df.columns if col in possible_names), None)\n",
        "                if found_col:\n",
        "                    column_mapping_found[required_col] = found_col\n",
        "\n",
        "            # Check if we found all required columns\n",
        "            missing = [col for col in required_cols.keys() if col not in column_mapping_found]\n",
        "\n",
        "            if missing:\n",
        "                print(f\"Aviso: {os.path.basename(file)} está faltando colunas {missing}\")\n",
        "                print(\"Colunas esperadas podem ter os seguintes nomes:\")\n",
        "                for col, alternatives in required_cols.items():\n",
        "                    if col in missing:\n",
        "                        print(f\"  - {col}: {alternatives}\")\n",
        "                continue\n",
        "\n",
        "            # Rename columns to standard names\n",
        "            df = df.rename(columns=dict((v, k) for k, v in column_mapping_found.items()))\n",
        "\n",
        "            # Adicionar identificador da estação\n",
        "            station_id = os.path.splitext(os.path.basename(file))[0]\n",
        "            df['station_id'] = station_id\n",
        "\n",
        "            # Remover linhas com valores nulos na temperatura\n",
        "            df = df.dropna(subset=['temperatura'])\n",
        "\n",
        "            # Verificar se as datas foram convertidas corretamente\n",
        "            if not pd.api.types.is_datetime64_any_dtype(df['Data']):\n",
        "                print(f\"Aviso: Falha ao converter datas em {os.path.basename(file)}\")\n",
        "                continue\n",
        "\n",
        "            dfs.append(df)\n",
        "            print(f\"Processado com sucesso: {len(df)} registros\")\n",
        "            print(f\"Intervalo de datas: {df['Data'].min()} até {df['Data'].max()}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao ler {file}: {str(e)}\")\n",
        "            print(\"Verifique se o arquivo está corrompido ou se tem o formato esperado\")\n",
        "            continue\n",
        "\n",
        "    if not dfs:\n",
        "        print(\"\\nErro: Nenhum arquivo válido para processar\")\n",
        "        print(\"Verifique se os arquivos Excel têm as colunas necessárias nos formatos aceitos:\")\n",
        "        for col, alternatives in required_cols.items():\n",
        "            print(f\"  - {col}: {alternatives}\")\n",
        "        raise ValueError(\"Nenhum arquivo válido para processar\")\n",
        "\n",
        "    # Combine all dataframes\n",
        "    print(\"\\nCombinando dados de todas as estações...\")\n",
        "    all_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Extrair características temporais da coluna Data\n",
        "    all_df['year'] = all_df['Data'].dt.year\n",
        "    all_df['month'] = all_df['Data'].dt.month\n",
        "    all_df['day'] = all_df['Data'].dt.day\n",
        "\n",
        "    # Selecionar apenas as colunas relevantes para o modelo\n",
        "    feature_cols = ['temperatura', 'year', 'month', 'day']\n",
        "    all_df = all_df[feature_cols]\n",
        "\n",
        "    # Update x_dim based on actual number of features\n",
        "    global x_dim\n",
        "    x_dim = len(feature_cols)\n",
        "    print(f\"Número de características (x_dim): {x_dim}\")\n",
        "    print(f\"Número total de registros: {len(all_df)}\")\n",
        "\n",
        "    return all_df\n",
        "\n",
        "def plot_loss_moment(history):\n",
        "    _, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
        "    ax.plot(history['loss'], 'blue', label='Loss', linewidth=1)\n",
        "    ax.plot(history['reconstruction_loss'], 'red', label='Reconstruction loss', linewidth=1)\n",
        "    ax.plot(history['kl_loss'], 'green', label='KL loss', linewidth=1)\n",
        "    ax.set_title('Loss and reconstruction loss over epochs')\n",
        "    ax.set_ylabel('Loss and reconstruction loss')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.legend(loc='upper right')\n",
        "    plt.savefig(image_dir + 'loss_lstm_vae_' + mode + '.png')\n",
        "\n",
        "def plot_log_likelihood(df_log_px):\n",
        "    plt.figure(figsize=(14, 6), dpi=80)\n",
        "    plt.title(\"Log likelihood\")\n",
        "    sns.set_color_codes()\n",
        "    sns.distplot(df_log_px, bins=40, kde=True, rug=True, color='blue')\n",
        "    plt.savefig(image_dir + 'log_likelihood_' + mode + '.png')\n",
        "\n",
        "def save_processing_summary(predictions, originals, log_px, threshold, model):\n",
        "    \"\"\"Salva um resumo do processamento em Excel\"\"\"\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    excel_path = os.path.join(results_dir, f'resumo_processamento_{timestamp}.xlsx')\n",
        "\n",
        "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
        "        # Aba 1: Dados de Reconstrução\n",
        "        df_reconstruction = pd.DataFrame({\n",
        "            'Original': originals.flatten(),\n",
        "            'Reconstruído': predictions.flatten(),\n",
        "            'Diferença': originals.flatten() - predictions.flatten()\n",
        "        })\n",
        "        df_reconstruction.to_excel(writer, sheet_name='Reconstrução', index=True)\n",
        "\n",
        "        # Aba 2: Métricas de Erro\n",
        "        errors = -np.mean(log_px, axis=0)\n",
        "        df_errors = pd.DataFrame({\n",
        "            'Log-likelihood': errors,\n",
        "            'Threshold': threshold,\n",
        "            'É Anomalia': errors > threshold\n",
        "        })\n",
        "        df_errors.to_excel(writer, sheet_name='Métricas', index=True)\n",
        "\n",
        "        # Aba 3: Configurações do Modelo\n",
        "        config_data = {\n",
        "            'Parâmetro': [\n",
        "                'Total Parâmetros',\n",
        "                'Parâmetros Treináveis',\n",
        "                'Batch Size',\n",
        "                'Threshold',\n",
        "                'Data Processamento',\n",
        "                'Diretório Modelo',\n",
        "                'Diretório Imagens',\n",
        "                'Diretório Resultados'\n",
        "            ],\n",
        "            'Valor': [\n",
        "                model.count_params(),\n",
        "                len(model.trainable_variables),\n",
        "                batch_size,\n",
        "                threshold,\n",
        "                datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                os.path.abspath(model_dir),\n",
        "                os.path.abspath(image_dir),\n",
        "                os.path.abspath(results_dir)\n",
        "            ]\n",
        "        }\n",
        "        pd.DataFrame(config_data).to_excel(writer, sheet_name='Configurações', index=False)\n",
        "\n",
        "        # Aba 4: Estatísticas\n",
        "        stats_data = {\n",
        "            'Métrica': [\n",
        "                'Média Original',\n",
        "                'Média Reconstruída',\n",
        "                'Desvio Padrão Original',\n",
        "                'Desvio Padrão Reconstruído',\n",
        "                'Erro Médio Absoluto',\n",
        "                'Erro Quadrático Médio',\n",
        "                'Total Anomalias',\n",
        "                '% Anomalias'\n",
        "            ],\n",
        "            'Valor': [\n",
        "                np.mean(originals),\n",
        "                np.mean(predictions),\n",
        "                np.std(originals),\n",
        "                np.std(predictions),\n",
        "                np.mean(np.abs(originals - predictions)),\n",
        "                np.mean((originals - predictions)**2),\n",
        "                np.sum(errors > threshold),\n",
        "                (np.sum(errors > threshold) / len(errors)) * 100\n",
        "            ]\n",
        "        }\n",
        "        pd.DataFrame(stats_data).to_excel(writer, sheet_name='Estatísticas', index=False)\n",
        "\n",
        "    print(f\"\\nResumo do processamento salvo em: {excel_path}\")\n",
        "    return excel_path\n",
        "\n",
        "def plot_training_results(model, train_dataset, threshold):\n",
        "    \"\"\"Plota os resultados do treinamento e salva resumo em Excel\"\"\"\n",
        "    # Coletar predições\n",
        "    all_predictions = []\n",
        "    all_originals = []\n",
        "    all_log_px = []\n",
        "\n",
        "    for batch in train_dataset:\n",
        "        pred = model(batch)\n",
        "        all_predictions.append(pred.numpy())\n",
        "        all_originals.append(batch.numpy())\n",
        "        _, _, log_px = model.predict(batch)\n",
        "        all_log_px.append(log_px.numpy())\n",
        "\n",
        "    predictions = np.concatenate(all_predictions)\n",
        "    originals = np.concatenate(all_originals)\n",
        "    log_px = np.concatenate(all_log_px)\n",
        "\n",
        "    # Salvar resumo em Excel\n",
        "    excel_path = save_processing_summary(predictions, originals, log_px, threshold, model)\n",
        "\n",
        "    # Criar figura com subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Reconstrução vs Original\n",
        "    ax1.plot(predictions[:100, 0, 0], label='Reconstruído', alpha=0.7)\n",
        "    ax1.plot(originals[:100, 0, 0], label='Original', alpha=0.7)\n",
        "    ax1.set_title('Comparação entre Dados Originais e Reconstruídos')\n",
        "    ax1.set_xlabel('Tempo')\n",
        "    ax1.set_ylabel('Valor')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot 2: Log-likelihood e Threshold\n",
        "    errors = -np.mean(log_px, axis=0)\n",
        "    ax2.plot(errors, label='Log-likelihood')\n",
        "    ax2.axhline(y=threshold, color='r', linestyle='--', label='Threshold')\n",
        "    ax2.set_title('Log-likelihood e Threshold')\n",
        "    ax2.set_xlabel('Amostra')\n",
        "    ax2.set_ylabel('Log-likelihood')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(image_dir, 'training_results.png'))\n",
        "    plt.close()\n",
        "\n",
        "    return excel_path\n",
        "\n",
        "def save_model(model):\n",
        "    \"\"\"Salva o modelo e seus pesos\"\"\"\n",
        "    # Criar diretório se não existir\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Salvar pesos do modelo\n",
        "    weights_path = os.path.join(model_dir, 'lstm_vae.weights.h5')\n",
        "    model.save_weights(weights_path)\n",
        "    print(f\"Modelo salvo em: {weights_path}\")\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"Carrega o modelo salvo\"\"\"\n",
        "    # Criar modelo com arquitetura padrão\n",
        "    model = LSTMVAE(\n",
        "        time_step=time_step,\n",
        "        x_dim=3,  # número de features\n",
        "        lstm_h_dim=16,  # dimensão padrão do LSTM\n",
        "        z_dim=8  # dimensão padrão do espaço latente\n",
        "    )\n",
        "\n",
        "    # Carregar pesos\n",
        "    weights_path = os.path.join(model_dir, 'lstm_vae.weights.h5')\n",
        "    if os.path.exists(weights_path):\n",
        "        model.load_weights(weights_path)\n",
        "        print(f\"Pesos carregados de: {weights_path}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Arquivo de pesos não encontrado em: {weights_path}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def save_model_and_scaler(model, scaler):\n",
        "    \"\"\"Salva o modelo, scaler e threshold\"\"\"\n",
        "    # Salvar modelo\n",
        "    save_model(model)\n",
        "\n",
        "    # Salvar scaler\n",
        "    scaler_path = os.path.join(model_dir, 'scaler.pkl')\n",
        "    with open(scaler_path, 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "    print(f\"Scaler salvo em: {scaler_path}\")\n",
        "\n",
        "    # Salvar threshold\n",
        "    threshold_path = os.path.join(model_dir, 'threshold.npy')\n",
        "    np.save(threshold_path, threshold)\n",
        "    print(f\"Threshold salvo em: {threshold_path}\")\n",
        "\n",
        "def main():\n",
        "    print(\"\\nIniciando processamento...\")\n",
        "    if using_gpu:\n",
        "        setup_gpu()\n",
        "\n",
        "    try:\n",
        "        print(\"\\nCarregando dados...\")\n",
        "        all_df = load_and_prepare_data()\n",
        "\n",
        "        print(\"\\nPreparando dados de treino...\")\n",
        "        train_dataset, test_dataset, scaler = prepare_training_data(all_df, batch_size=batch_size)\n",
        "\n",
        "        if mode == \"train\":\n",
        "            print(\"\\nIniciando otimização de hiperparâmetros...\")\n",
        "            model = hyperparameter_tuning(train_dataset)\n",
        "            model.summary()\n",
        "\n",
        "            # Calcular threshold dinâmico (95º percentil dos erros de treino)\n",
        "            all_train_log_px = []\n",
        "            for batch in train_dataset:\n",
        "                _, _, train_log_px = model.predict(batch)\n",
        "                all_train_log_px.append(train_log_px.numpy())\n",
        "\n",
        "            train_errors = -np.mean(np.concatenate(all_train_log_px), axis=0)\n",
        "            global threshold\n",
        "            threshold = np.percentile(train_errors, 95)\n",
        "\n",
        "            print(f\"Threshold calculado: {threshold}\")\n",
        "\n",
        "            # Salvar modelo e scaler\n",
        "            print(\"Salvando modelo e scaler...\")\n",
        "            save_model_and_scaler(model, scaler)\n",
        "\n",
        "            # Plotar resultados do treinamento\n",
        "            print(\"Gerando visualizações...\")\n",
        "            plot_training_results(model, train_dataset, threshold)\n",
        "\n",
        "            print(\"\\nTreinamento concluído com sucesso!\")\n",
        "            print(f\"Modelo salvo em: {model_dir}\")\n",
        "            print(f\"Visualizações salvas em: {image_dir}\")\n",
        "\n",
        "        elif mode == \"infer\":\n",
        "            model = load_model()\n",
        "            model.compile(optimizer=optimizers.Adam(learning_rate=0.001, epsilon=1e-6, amsgrad=True))\n",
        "\n",
        "            # Carregar threshold\n",
        "            threshold = np.load(os.path.join(model_dir, 'threshold.npy'))\n",
        "            print(f\"Threshold carregado: {threshold:.4f}\")\n",
        "\n",
        "            # Fazer predições\n",
        "            all_log_px = []\n",
        "            for batch in train_dataset:\n",
        "                _, _, log_px = model.predict(batch)\n",
        "                all_log_px.append(log_px.numpy())\n",
        "\n",
        "            train_log_px = np.concatenate(all_log_px)\n",
        "            train_errors = -np.mean(train_log_px, axis=0)\n",
        "\n",
        "            # Criar DataFrame com resultados\n",
        "            df_train_log_px = pd.DataFrame()\n",
        "            df_train_log_px['log_likelihood'] = train_errors\n",
        "            df_train_log_px['anomaly'] = df_train_log_px['log_likelihood'].apply(\n",
        "                lambda x: 1 if x > threshold else 0\n",
        "            )\n",
        "\n",
        "            # Plotar resultados\n",
        "            plot_log_likelihood(df_train_log_px, mode='train')\n",
        "\n",
        "            # Mostrar estatísticas das anomalias\n",
        "            n_anomalies = df_train_log_px['anomaly'].sum()\n",
        "            print(f\"\\nEstatísticas das Anomalias:\")\n",
        "            print(f\"Total de anomalias detectadas: {n_anomalies}\")\n",
        "            print(f\"Porcentagem de anomalias: {(n_anomalies/len(df_train_log_px))*100:.2f}%\")\n",
        "\n",
        "            print(\"\\nProcesso de inferência concluído!\")\n",
        "\n",
        "        else:\n",
        "            print(f\"\\nModo {mode} não reconhecido!\")\n",
        "            exit(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nErro durante a execução: {str(e)}\")\n",
        "        raise e\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "7J98WncW5ev_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}