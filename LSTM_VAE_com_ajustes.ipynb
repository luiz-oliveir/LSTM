{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxm7eQQWv6FUd5XSpDUZy/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiz-oliveir/LSTM/blob/main/LSTM_VAE_com_ajustes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import datetime\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configurações\n",
        "row_mark = 740\n",
        "batch_size = 128\n",
        "time_step = 1\n",
        "x_dim = 4\n",
        "lstm_h_dim = 8\n",
        "z_dim = 4\n",
        "epoch_num = 100\n",
        "threshold = None\n",
        "\n",
        "# Diretórios\n",
        "model_dir = \"./lstm_vae_model/\"\n",
        "image_dir = \"./lstm_vae_images/\"\n",
        "results_dir = 'C:/Users/Augusto-PC/Documents/GitHub/LSTM/Resumo resultados/'\n",
        "mensal_dir = 'C:/Users/Augusto-PC/Documents/GitHub/LSTM/LSTM_mensal'\n",
        "\n",
        "# Criar diretórios\n",
        "for d in [model_dir, image_dir, results_dir, mensal_dir]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def split_normalize_data(all_df):\n",
        "    train_df = all_df[:row_mark]\n",
        "    test_df = all_df[row_mark:]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(np.array(all_df)[:, 1:])\n",
        "    train_scaled = scaler.transform(np.array(train_df)[:, 1:])\n",
        "    test_scaled = scaler.transform(np.array(test_df)[:, 1:])\n",
        "    return train_scaled, test_scaled, scaler\n",
        "\n",
        "def reshape(da):\n",
        "    return da.reshape(da.shape[0], time_step, da.shape[1]).astype(\"float32\")\n",
        "\n",
        "def prepare_training_data(data, batch_size=128):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "    dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "def save_monthly_results(predictions, originals, log_px, df_original, file_name):\n",
        "    \"\"\"Salva os resultados mensais em diferentes sheets\"\"\"\n",
        "    # Preparar dados\n",
        "    df_original['Data'] = pd.to_datetime(df_original['Data'])\n",
        "    df_original['Mes'] = df_original['Data'].dt.month\n",
        "\n",
        "    # Nome do arquivo\n",
        "    output_file = os.path.join(mensal_dir, f'analise_mensal_{os.path.basename(file_name)}')\n",
        "\n",
        "    # Dicionário de meses\n",
        "    meses = {\n",
        "        1:'jan', 2:'fev', 3:'mar', 4:'abr', 5:'mai', 6:'jun',\n",
        "        7:'jul', 8:'ago', 9:'set', 10:'out', 11:'nov', 12:'dez'\n",
        "    }\n",
        "\n",
        "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
        "        for mes, nome_mes in meses.items():\n",
        "            mask_mes = df_original['Mes'] == mes\n",
        "            if mask_mes.any():\n",
        "                indices = df_original[mask_mes].index\n",
        "\n",
        "                df_mes = pd.DataFrame()\n",
        "                df_mes['Data'] = df_original.loc[indices, 'Data']\n",
        "                df_mes['Log_Likelihood'] = log_px[indices].flatten()\n",
        "\n",
        "                # Adicionar predições e valores originais\n",
        "                for i in range(predictions.shape[2]):\n",
        "                    df_mes[f'Predição_{i+1}'] = predictions[indices, 0, i]\n",
        "                    df_mes[f'Original_{i+1}'] = originals[indices, 0, i]\n",
        "\n",
        "                # Adicionar métricas\n",
        "                df_mes['Erro_Médio'] = np.mean(np.abs(\n",
        "                    predictions[indices, 0, :] - originals[indices, 0, :]), axis=1)\n",
        "\n",
        "                # Estatísticas mensais\n",
        "                stats = pd.DataFrame({\n",
        "                    'Métrica': ['Média Original', 'Média Predição', 'Erro Médio'],\n",
        "                    'Valor': [\n",
        "                        np.mean(originals[indices]),\n",
        "                        np.mean(predictions[indices]),\n",
        "                        np.mean(df_mes['Erro_Médio'])\n",
        "                    ]\n",
        "                })\n",
        "\n",
        "                # Salvar na sheet do mês\n",
        "                with pd.ExcelWriter(output_file, engine='openpyxl', mode='a') as writer:\n",
        "                    df_mes.to_excel(writer, sheet_name=f'{nome_mes}_dados', index=False)\n",
        "                    stats.to_excel(writer, sheet_name=f'{nome_mes}_stats', index=False)\n",
        "\n",
        "                print(f\"Dados salvos para {nome_mes}\")\n",
        "\n",
        "    print(f\"\\nAnálise mensal salva em: {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "def process_file(file_path):\n",
        "    \"\"\"Processa um único arquivo\"\"\"\n",
        "    print(f\"\\nProcessando arquivo: {file_path}\")\n",
        "\n",
        "    try:\n",
        "        # Carregar dados\n",
        "        df = pd.read_excel(file_path, parse_dates=['Data'])\n",
        "\n",
        "        # Normalizar dados\n",
        "        train_scaled, test_scaled, scaler = split_normalize_data(df)\n",
        "\n",
        "        # Preparar dataset\n",
        "        train_dataset = prepare_training_data(train_scaled)\n",
        "\n",
        "        # Carregar modelo\n",
        "        model = tf.keras.models.load_model(os.path.join(model_dir, 'lstm_vae_model.h5'))\n",
        "\n",
        "        # Fazer predições\n",
        "        predictions = model.predict(reshape(train_scaled))\n",
        "        originals = reshape(train_scaled)\n",
        "        log_px = model.predict(reshape(train_scaled))[2]\n",
        "\n",
        "        # Salvar resultados mensais\n",
        "        save_monthly_results(predictions, originals, log_px, df, file_path)\n",
        "\n",
        "        print(f\"Processamento concluído para {file_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao processar {file_path}: {str(e)}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Função principal\"\"\"\n",
        "    # Ler diretório de dados\n",
        "    try:\n",
        "        with open('data_path.txt', 'r') as f:\n",
        "            data_dir = f.read().strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao ler data_path.txt: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Procurar arquivos Excel\n",
        "    excel_files = [f for f in glob.glob(os.path.join(data_dir, \"*.xlsx\"))\n",
        "                  if not os.path.basename(f).startswith('~$')]\n",
        "\n",
        "    if not excel_files:\n",
        "        print(\"Nenhum arquivo Excel encontrado!\")\n",
        "        return\n",
        "\n",
        "    # Processar cada arquivo\n",
        "    for file in excel_files:\n",
        "        process_file(file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "7J98WncW5ev_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}