{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOf/oFOsMWfVZgeoMjL440I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiz-oliveir/LSTM/blob/main/setup_and_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "def run_command(cmd, check=True, shell=False):\n",
        "    \"\"\"Helper function to run commands and handle errors\"\"\"\n",
        "    try:\n",
        "        print(f\"Executando comando: {' '.join(str(x) for x in cmd)}\")\n",
        "        result = subprocess.run(cmd, check=check, shell=shell,\n",
        "                            stdout=subprocess.PIPE,\n",
        "                            stderr=subprocess.PIPE,\n",
        "                            universal_newlines=True)\n",
        "        print(f\"Output: {result.stdout}\")\n",
        "        return result\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Command failed: {' '.join(str(x) for x in cmd)}\")\n",
        "        print(f\"Error output: {e.stderr}\")\n",
        "        return None\n",
        "\n",
        "def clone_repository():\n",
        "    \"\"\"Clone the GitHub repository if in Colab\"\"\"\n",
        "    if IN_COLAB:\n",
        "        # Change to /content directory\n",
        "        os.chdir('/content')\n",
        "\n",
        "        # Remove existing directory if it exists\n",
        "        if os.path.exists('/content/LSTM'):\n",
        "            print(\"Removing existing repository...\")\n",
        "            shutil.rmtree('/content/LSTM')\n",
        "\n",
        "        print(\"Cloning repository...\")\n",
        "        # Clone the repository\n",
        "        cmd = [\"git\", \"clone\", \"https://github.com/luiz-oliveir/LSTM.git\"]\n",
        "        result = run_command(cmd)\n",
        "\n",
        "        if result is None or result.returncode != 0:\n",
        "            print(\"Failed to clone repository\")\n",
        "            return False\n",
        "\n",
        "        print(\"Repository cloned successfully\")\n",
        "        return True\n",
        "    return True\n",
        "\n",
        "def get_project_dir():\n",
        "    \"\"\"Get the project directory in Google Colab or local environment\"\"\"\n",
        "    if IN_COLAB:\n",
        "        project_dir = Path('/content/LSTM')\n",
        "    else:\n",
        "        project_dir = Path(os.getcwd()).parent\n",
        "\n",
        "    print(f\"Checking Colab path: {project_dir}\")\n",
        "    print(f\"Path exists: {project_dir.exists()}\")\n",
        "\n",
        "    if not project_dir.exists():\n",
        "        print(\"Project directory not found\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Current directory: {project_dir}\")\n",
        "    print(f\"Directory contents: {[f.name for f in project_dir.iterdir()]}\")\n",
        "\n",
        "    return project_dir\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"Setup the Python environment with required packages\"\"\"\n",
        "    if not clone_repository():\n",
        "        return False\n",
        "\n",
        "    project_dir = get_project_dir()\n",
        "    if not project_dir:\n",
        "        print(\"Failed to locate project directory\")\n",
        "        return False\n",
        "\n",
        "    if IN_COLAB:\n",
        "        print(\"Installing requirements...\")\n",
        "        requirements_file = project_dir / 'requirements.txt'\n",
        "\n",
        "        # Create requirements.txt if it doesn't exist\n",
        "        if not requirements_file.exists():\n",
        "            print(\"Creating requirements.txt...\")\n",
        "            requirements = \"\"\"tensorflow>=2.8.0\n",
        "numpy>=1.19.2\n",
        "pandas>=1.3.0\n",
        "scikit-learn>=0.24.2\n",
        "keras-tuner>=1.1.0\n",
        "matplotlib>=3.4.3\"\"\"\n",
        "            with open(requirements_file, 'w') as f:\n",
        "                f.write(requirements)\n",
        "            print(\"Created requirements.txt\")\n",
        "\n",
        "        print(\"Installing dependencies...\")\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(requirements_file)]\n",
        "        result = run_command(cmd)\n",
        "        if result is None or result.returncode != 0:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def run_notebook():\n",
        "    \"\"\"Run the main notebook\"\"\"\n",
        "    try:\n",
        "        print(\"Executing LSTM VAE notebook...\")\n",
        "        cmd = [\"jupyter\", \"nbconvert\",\n",
        "               \"--to\", \"notebook\",\n",
        "               \"--execute\",\n",
        "               \"--inplace\",\n",
        "               \"LSTM_VAE_com_ajustes.ipynb\"]\n",
        "        result = run_command(cmd)\n",
        "        if result and result.returncode == 0:\n",
        "            print(\"Successfully executed notebook\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Error executing notebook\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error running notebook: {e}\")\n",
        "        return False\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if setup_environment():\n",
        "        project_dir = get_project_dir()\n",
        "        if project_dir:\n",
        "            os.chdir(project_dir)\n",
        "            run_notebook()\n",
        "        else:\n",
        "            print(\"Failed to setup environment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcU3sYuxQXfg",
        "outputId": "5c231fae-6fe1-4a1c-f6b6-de2d61f9b205"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing existing repository...\n",
            "Cloning repository...\n",
            "Executando comando: git clone https://github.com/luiz-oliveir/LSTM.git\n",
            "Output: \n",
            "Repository cloned successfully\n",
            "Checking Colab path: /content/LSTM\n",
            "Path exists: True\n",
            "Current directory: /content/LSTM\n",
            "Directory contents: ['.git', 'setup_and_run.ipynb', 'LSTM_VAE_com_ajustes.ipynb', 'setup_and_run-checkpoint.py', 'lstm_vae-checkpoint.py', 'Leiame.txt', 'README.md', 'LSTM_VAE com ajustes-checkpoint.py']\n",
            "Installing requirements...\n",
            "Creating requirements.txt...\n",
            "Created requirements.txt\n",
            "Installing dependencies...\n",
            "Executando comando: /usr/bin/python3 -m pip install -r /content/LSTM/requirements.txt\n",
            "Output: Requirement already satisfied: tensorflow>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/LSTM/requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.19.2 in /usr/local/lib/python3.11/dist-packages (from -r /content/LSTM/requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/LSTM/requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from -r /content/LSTM/requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: keras-tuner>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/LSTM/requirements.txt (line 5)) (1.4.7)\n",
            "Requirement already satisfied: matplotlib>=3.4.3 in /usr/local/lib/python3.11/dist-packages (from -r /content/LSTM/requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->-r /content/LSTM/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->-r /content/LSTM/requirements.txt (line 3)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->-r /content/LSTM/requirements.txt (line 3)) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.2->-r /content/LSTM/requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.2->-r /content/LSTM/requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.2->-r /content/LSTM/requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.11/dist-packages (from keras-tuner>=1.1.0->-r /content/LSTM/requirements.txt (line 5)) (1.0.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.4.3->-r /content/LSTM/requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.4.3->-r /content/LSTM/requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.4.3->-r /content/LSTM/requirements.txt (line 6)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.4.3->-r /content/LSTM/requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.4.3->-r /content/LSTM/requirements.txt (line 6)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.4.3->-r /content/LSTM/requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.8.0->-r /content/LSTM/requirements.txt (line 1)) (0.1.2)\n",
            "\n",
            "Checking Colab path: /content/LSTM\n",
            "Path exists: True\n",
            "Current directory: /content/LSTM\n",
            "Directory contents: ['.git', 'setup_and_run.ipynb', 'LSTM_VAE_com_ajustes.ipynb', 'setup_and_run-checkpoint.py', 'lstm_vae-checkpoint.py', 'requirements.txt', 'Leiame.txt', 'README.md', 'LSTM_VAE com ajustes-checkpoint.py']\n",
            "Executing LSTM VAE notebook...\n",
            "Executando comando: jupyter nbconvert --to notebook --execute --inplace LSTM_VAE_com_ajustes.ipynb\n",
            "Command failed: jupyter nbconvert --to notebook --execute --inplace LSTM_VAE_com_ajustes.ipynb\n",
            "Error output: [NbConvertApp] Converting notebook LSTM_VAE_com_ajustes.ipynb to notebook\n",
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "2025-02-28 20:32:55.941446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740774775.975721   20191 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740774775.988016   20191 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-28 20:32:56.023861: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-02-28 20:32:59.895398: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Exception ignored in atexit callback: <function shutdown at 0x78d6aa5d0a40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 2186, in shutdown\n",
            "    h.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/absl/logging/__init__.py\", line 944, in close\n",
            "    self.stream.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/iostream.py\", line 446, in close\n",
            "    self.watch_fd_thread.join()\n",
            "    ^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'OutStream' object has no attribute 'watch_fd_thread'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/jupyter-nbconvert\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jupyter_core/application.py\", line 283, in launch_instance\n",
            "    super().launch_instance(argv=argv, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/nbconvertapp.py\", line 420, in start\n",
            "    self.convert_notebooks()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/nbconvertapp.py\", line 597, in convert_notebooks\n",
            "    self.convert_single_notebook(notebook_filename)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/nbconvertapp.py\", line 563, in convert_single_notebook\n",
            "    output, resources = self.export_single_notebook(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/nbconvertapp.py\", line 487, in export_single_notebook\n",
            "    output, resources = self.exporter.from_filename(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/exporters/exporter.py\", line 201, in from_filename\n",
            "    return self.from_file(f, resources=resources, **kw)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/exporters/exporter.py\", line 220, in from_file\n",
            "    return self.from_notebook_node(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/exporters/notebook.py\", line 36, in from_notebook_node\n",
            "    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/exporters/exporter.py\", line 154, in from_notebook_node\n",
            "    nb_copy, resources = self._preprocess(nb_copy, resources)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/exporters/exporter.py\", line 353, in _preprocess\n",
            "    nbc, resc = preprocessor(nbc, resc)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/preprocessors/base.py\", line 48, in __call__\n",
            "    return self.preprocess(nb, resources)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/preprocessors/execute.py\", line 103, in preprocess\n",
            "    self.preprocess_cell(cell, resources, index)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/preprocessors/execute.py\", line 124, in preprocess_cell\n",
            "    cell = self.execute_cell(cell, index, store_history=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jupyter_core/utils/__init__.py\", line 165, in wrapped\n",
            "    return loop.run_until_complete(inner)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n",
            "    return future.result()\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbclient/client.py\", line 1062, in async_execute_cell\n",
            "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nbclient/client.py\", line 918, in _check_raise_for_error\n",
            "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
            "nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n",
            "------------------\n",
            "import os\n",
            "import sys\n",
            "\n",
            "# Configure matplotlib backend based on environment\n",
            "if 'ipykernel' in sys.modules:\n",
            "    # Running in Jupyter/IPython\n",
            "    try:\n",
            "        import IPython\n",
            "        ipython = IPython.get_ipython()\n",
            "        ipython.run_line_magic('matplotlib', 'inline')\n",
            "    except Exception:\n",
            "        os.environ['MPLBACKEND'] = 'TkAgg'\n",
            "else:\n",
            "    # Running as script\n",
            "    os.environ['MPLBACKEND'] = 'TkAgg'\n",
            "\n",
            "import numpy as np\n",
            "np.random.seed(0)\n",
            "import pandas as pd\n",
            "from sklearn.preprocessing import MinMaxScaler\n",
            "from sklearn.model_selection import train_test_split\n",
            "import tensorflow as tf\n",
            "tf.random.set_seed(0)\n",
            "import glob\n",
            "from keras_tuner import RandomSearch\n",
            "import sys\n",
            "\n",
            "# Configuração otimizada para GPU NVIDIA/CUDA\n",
            "def setup_gpu():\n",
            "    print(\"\\nVerificando configuração GPU:\")\n",
            "    print(\"TensorFlow versão:\", tf.__version__)\n",
            "    print(\"CUDA disponível:\", tf.test.is_built_with_cuda())\n",
            "    print(\"GPU disponível para TensorFlow:\", tf.test.is_gpu_available())\n",
            "\n",
            "    try:\n",
            "        # Listar GPUs disponíveis\n",
            "        gpus = tf.config.list_physical_devices('GPU')\n",
            "        if gpus:\n",
            "            print(\"\\nGPUs disponíveis:\", len(gpus))\n",
            "            for gpu in gpus:\n",
            "                print(\" -\", gpu.name)\n",
            "\n",
            "            # Permitir crescimento de memória dinâmico\n",
            "            for gpu in gpus:\n",
            "                tf.config.experimental.set_memory_growth(gpu, True)\n",
            "\n",
            "            # Configurar para formato de dados mixed precision\n",
            "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
            "            tf.keras.mixed_precision.set_global_policy(policy)\n",
            "\n",
            "            print(\"\\nGPU configurada com sucesso!\")\n",
            "            print(\"Usando mixed precision:\", policy.name)\n",
            "            return True\n",
            "        else:\n",
            "            print(\"\\nNenhuma GPU encontrada. Usando CPU.\")\n",
            "            return False\n",
            "\n",
            "    except Exception as e:\n",
            "        print(\"\\nErro ao configurar GPU:\", str(e))\n",
            "        print(\"Usando CPU como fallback.\")\n",
            "        return False\n",
            "\n",
            "# Configurar GPU no início do script\n",
            "using_gpu = setup_gpu()\n",
            "\n",
            "from tensorflow import keras, data\n",
            "import tensorflow_probability as tfp\n",
            "from tensorflow.keras import layers, regularizers, activations, optimizers\n",
            "from tensorflow.keras import backend as K\n",
            "import seaborn as sns\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "dataset_name = \"bearing_dataset\"  # Apenas para referência\n",
            "#train_ratio = 0.75\n",
            "row_mark = 740\n",
            "batch_size = 128\n",
            "time_step = 1\n",
            "x_dim = 4\n",
            "lstm_h_dim = 8\n",
            "z_dim = 4\n",
            "epoch_num = 100\n",
            "threshold = None\n",
            "\n",
            "mode = 'train'\n",
            "model_dir = \"./lstm_vae_model/\"\n",
            "image_dir = \"./lstm_vae_images/\"\n",
            "data_dir = r\"C:\\F_analises\\INMET\\Convencionais processadas temperaturas\"\n",
            "\n",
            "# Criar diretórios necessários\n",
            "os.makedirs(model_dir, exist_ok=True)\n",
            "os.makedirs(image_dir, exist_ok=True)\n",
            "os.makedirs(data_dir, exist_ok=True)\n",
            "\n",
            "# Parâmetros de ativação\n",
            "lstm_activation = 'softplus'  # Pode mudar para 'tanh', 'relu', etc\n",
            "sigma_activation = 'tanh'     # Ativação para sigma_x\n",
            "\n",
            "def split_normalize_data(all_df):\n",
            "    #row_mark = int(all_df.shape[0] * train_ratio)\n",
            "    train_df = all_df[:row_mark]\n",
            "    test_df = all_df[row_mark:]\n",
            "\n",
            "    scaler = MinMaxScaler()\n",
            "    scaler.fit(np.array(all_df)[:, 1:])\n",
            "    train_scaled = scaler.transform(np.array(train_df)[:, 1:])\n",
            "    test_scaled = scaler.transform(np.array(test_df)[:, 1:])\n",
            "    return train_scaled, test_scaled\n",
            "\n",
            "def reshape(da):\n",
            "    return da.reshape(da.shape[0], time_step, da.shape[1]).astype(\"float32\")\n",
            "\n",
            "class Sampling(layers.Layer):\n",
            "    def __init__(self, name='sampling_z'):\n",
            "        super(Sampling, self).__init__(name=name)\n",
            "\n",
            "    def call(self, inputs):\n",
            "        mu, logvar = inputs\n",
            "        print('mu: ', mu)\n",
            "        sigma = K.exp(logvar * 0.5)\n",
            "        epsilon = K.random_normal(shape=(mu.shape[0], z_dim), mean=0.0, stddev=1.0)\n",
            "        return mu + epsilon * sigma\n",
            "\n",
            "    def get_config(self):\n",
            "        config = super(Sampling, self).get_config()\n",
            "        config.update({'name': self.name})\n",
            "        return config\n",
            "\n",
            "class Encoder(layers.Layer):\n",
            "    def __init__(self, time_step, x_dim, lstm_h_dim, z_dim, name='encoder', activation=lstm_activation, **kwargs):\n",
            "        super(Encoder, self).__init__(name=name, **kwargs)\n",
            "\n",
            "        self.encoder_inputs = keras.Input(shape=(time_step, x_dim))\n",
            "        self.encoder_lstm = layers.LSTM(\n",
            "            lstm_h_dim,\n",
            "            activation=activation,  # Usar parâmetro\n",
            "            name='encoder_lstm',\n",
            "            stateful=True\n",
            "        )\n",
            "        self.z_mean = layers.Dense(z_dim, name='z_mean')\n",
            "        self.z_logvar = layers.Dense(z_dim, name='z_log_var')\n",
            "        self.z_sample = Sampling()\n",
            "\n",
            "    def call(self, inputs):\n",
            "        self.encoder_inputs = inputs\n",
            "        hidden = self.encoder_lstm(self.encoder_inputs)\n",
            "        mu_z = self.z_mean(hidden)\n",
            "        logvar_z = self.z_logvar(hidden)\n",
            "        z = self.z_sample((mu_z, logvar_z))\n",
            "        return mu_z, logvar_z, z\n",
            "\n",
            "    def get_config(self):\n",
            "        config = super(Encoder, self).get_config()\n",
            "        config.update({\n",
            "            'name': self.name,\n",
            "            'z_sample': self.z_sample.get_config()\n",
            "        })\n",
            "        return config\n",
            "\n",
            "class Decoder(layers.Layer):\n",
            "    def __init__(self, time_step, x_dim, lstm_h_dim, z_dim, name='decoder', activation=lstm_activation, sigma_activation=sigma_activation, **kwargs):\n",
            "        super(Decoder, self).__init__(name=name, **kwargs)\n",
            "\n",
            "        self.z_inputs = layers.RepeatVector(time_step, name='repeat_vector')\n",
            "        self.decoder_lstm_hidden = layers.LSTM(\n",
            "            lstm_h_dim,\n",
            "            activation=activation,\n",
            "            return_sequences=True,\n",
            "            name='decoder_lstm'\n",
            "        )\n",
            "        self.x_mean = layers.Dense(x_dim, name='x_mean')\n",
            "        self.x_sigma = layers.Dense(\n",
            "            x_dim,\n",
            "            name='x_sigma',\n",
            "            activation=sigma_activation  # Usar parâmetro\n",
            "        )\n",
            "\n",
            "    def call(self, inputs):\n",
            "        z = self.z_inputs(inputs)\n",
            "        hidden = self.decoder_lstm_hidden(z)\n",
            "        mu_x = self.x_mean(hidden)\n",
            "        sigma_x = self.x_sigma(hidden)\n",
            "        return mu_x, sigma_x\n",
            "\n",
            "    def get_config(self):\n",
            "        config = super(Decoder, self).get_config()\n",
            "        config.update({\n",
            "            'name': self.name\n",
            "        })\n",
            "        return config\n",
            "\n",
            "loss_metric = keras.metrics.Mean(name='loss')\n",
            "likelihood_metric = keras.metrics.Mean(name='log likelihood')\n",
            "\n",
            "class LSTM_VAE(keras.Model):\n",
            "    def __init__(self, time_step, x_dim, lstm_h_dim, z_dim, name='lstm_vae', **kwargs):\n",
            "        super(LSTM_VAE, self).__init__(name=name, **kwargs)\n",
            "\n",
            "        self.encoder = Encoder(time_step, x_dim, lstm_h_dim, z_dim, **kwargs)\n",
            "        self.decoder = Decoder(time_step, x_dim, lstm_h_dim, z_dim, **kwargs)\n",
            "\n",
            "    def call(self, inputs):\n",
            "        mu_z, logvar_z, z = self.encoder(inputs)\n",
            "        mu_x, sigma_x = self.decoder(z)\n",
            "\n",
            "        var_z = K.exp(logvar_z)\n",
            "        kl_loss = K.mean(-0.5 * K.sum(var_z - logvar_z + tf.square(1 - mu_z), axis=1), axis=0)\n",
            "        self.add_loss(kl_loss)\n",
            "\n",
            "        dist = tfp.distributions.Normal(loc=mu_x, scale=tf.abs(sigma_x))\n",
            "        log_px = -dist.log_prob(inputs)\n",
            "\n",
            "        return mu_x, sigma_x, log_px\n",
            "\n",
            "    def get_config(self):\n",
            "        config = {\n",
            "            'encoder': self.encoder.get_config(),\n",
            "            'decoder': self.decoder.get_config(),\n",
            "            'name': self.name\n",
            "        }\n",
            "        return config\n",
            "\n",
            "    def reconstruct_loss(self, x, mu_x, sigma_x):\n",
            "        var_x = K.square(sigma_x)\n",
            "        reconst_loss = -0.5 * K.sum(K.log(var_x), axis=2) + K.sum(K.square(x - mu_x) / var_x, axis=2)\n",
            "        reconst_loss = K.reshape(reconst_loss, shape=(x.shape[0], 1))\n",
            "        return K.mean(reconst_loss, axis=0)\n",
            "\n",
            "    def mean_log_likelihood(self, log_px):\n",
            "        log_px = K.reshape(log_px, shape=(log_px.shape[0], log_px.shape[2]))\n",
            "        mean_log_px = K.mean(log_px, axis=1)\n",
            "        return K.mean(mean_log_px, axis=0)\n",
            "\n",
            "    def train_step(self, data):\n",
            "        with tf.device('/GPU:0' if using_gpu else '/CPU:0'):\n",
            "            with tf.GradientTape() as tape:\n",
            "                # Usar tf.data para otimizar o pipeline de dados\n",
            "                if isinstance(data, tf.data.Dataset):\n",
            "                    x = next(iter(data))\n",
            "                else:\n",
            "                    x = data\n",
            "\n",
            "                # Forward pass com mixed precision\n",
            "                if using_gpu:\n",
            "                    x = tf.cast(x, tf.float16)\n",
            "\n",
            "                # Forward pass\n",
            "                mu_z, logvar_z = self.encoder(x)\n",
            "                z = self.encoder.z_sample([mu_z, logvar_z])\n",
            "                mu_x, sigma_x = self.decoder(z)\n",
            "\n",
            "                # Calcular loss\n",
            "                log_px = self.reconstruct_loss(x, mu_x, sigma_x)\n",
            "                kl_loss = -0.5 * tf.reduce_mean(1 + logvar_z - tf.square(mu_z) - tf.exp(logvar_z))\n",
            "                loss = -log_px + kl_loss\n",
            "\n",
            "                # Escalar loss para mixed precision\n",
            "                if using_gpu:\n",
            "                    loss = tf.cast(loss, tf.float32)\n",
            "\n",
            "                # Atualizar métricas\n",
            "                loss_metric.update_state(loss)\n",
            "                likelihood_metric.update_state(log_px)\n",
            "\n",
            "            # Backpropagation otimizado\n",
            "            variables = self.trainable_variables\n",
            "            gradients = tape.gradient(loss, variables)\n",
            "            # Clipar gradientes para evitar explosão\n",
            "            gradients = [tf.clip_by_norm(g, 1.0) if g is not None else g for g in gradients]\n",
            "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
            "\n",
            "            return {\n",
            "                \"loss\": loss_metric.result(),\n",
            "                \"log_px\": likelihood_metric.result(),\n",
            "                \"kl_loss\": kl_loss\n",
            "            }\n",
            "\n",
            "def prepare_training_data(all_df, batch_size=128):  # Aumentado batch_size para GPU\n",
            "    # Preparar dados usando tf.data para pipeline otimizado\n",
            "    dataset = tf.data.Dataset.from_tensor_slices(all_df.values.astype('float32'))\n",
            "\n",
            "    # Configurar pipeline otimizado\n",
            "    dataset = dataset.cache()\n",
            "    dataset = dataset.shuffle(buffer_size=min(len(all_df), 10000))  # Buffer size limitado para memória\n",
            "    dataset = dataset.batch(batch_size)\n",
            "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
            "\n",
            "    if using_gpu:\n",
            "        # Usar estratégia de distribuição para GPU\n",
            "        strategy = tf.distribute.MirroredStrategy()\n",
            "        dataset = strategy.experimental_distribute_dataset(dataset)\n",
            "\n",
            "    return dataset\n",
            "\n",
            "def generate_sample_data():\n",
            "    \"\"\"Generate sample data if no Excel files are found.\"\"\"\n",
            "    print(\"\\nGenerating sample data for testing...\")\n",
            "\n",
            "    # Create sample timestamps\n",
            "    dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='H')\n",
            "    n_samples = len(dates)\n",
            "\n",
            "    # Generate synthetic data\n",
            "    np.random.seed(42)\n",
            "    data = {\n",
            "        'timestamp': dates,\n",
            "        'nivel_agua': np.random.normal(100, 10, n_samples),  # Normal distribution\n",
            "        'vazao': np.abs(np.random.normal(50, 5, n_samples)),  # Positive values\n",
            "        'temperatura': np.random.normal(25, 3, n_samples),  # Normal distribution\n",
            "    }\n",
            "\n",
            "    # Create DataFrame\n",
            "    df = pd.DataFrame(data)\n",
            "\n",
            "    # Add some anomalies\n",
            "    anomaly_idx = np.random.choice(n_samples, size=int(n_samples * 0.05), replace=False)\n",
            "    df.loc[anomaly_idx, 'nivel_agua'] *= 1.5\n",
            "    df.loc[anomaly_idx, 'vazao'] *= 2\n",
            "    df.loc[anomaly_idx, 'temperatura'] += 10\n",
            "\n",
            "    # Save to Excel file\n",
            "    sample_file = os.path.join(data_dir, 'sample_data.xlsx')\n",
            "    print(f\"Saving sample data to {sample_file}\")\n",
            "    df.to_excel(sample_file, index=False)\n",
            "    return df\n",
            "\n",
            "def load_and_prepare_data():\n",
            "    print(\"Loading and preparing data...\")\n",
            "\n",
            "    # Diretório onde estão os arquivos Excel\n",
            "    dfs = []\n",
            "\n",
            "    # Get list of Excel files (ignorando arquivos temporários que começam com ~$)\n",
            "    excel_files = [f for f in glob.glob(os.path.join(data_dir, \"*.xlsx\")) if not os.path.basename(f).startswith('~$')]\n",
            "\n",
            "    if not excel_files:\n",
            "        print(f\"\\nWARNING: No Excel files found in {data_dir}\")\n",
            "        print(\"Please make sure the Excel files are in the correct directory.\")\n",
            "        sys.exit(1)\n",
            "    else:\n",
            "        # Load and combine data from all files\n",
            "        for file in excel_files:\n",
            "            print(f\"Loading {os.path.basename(file)}...\")\n",
            "            try:\n",
            "                df = pd.read_excel(file)\n",
            "\n",
            "                # Verificar se as colunas necessárias existem\n",
            "                required_cols = ['Dia', 'Mes', 'Ano']\n",
            "                missing = [col for col in required_cols if col not in df.columns]\n",
            "\n",
            "                # Procurar a coluna de temperatura (pode ter variações no nome)\n",
            "                temp_col = None\n",
            "                for col in df.columns:\n",
            "                    if 'TEMPERATURA MAXIMA' in str(col).upper():\n",
            "                        temp_col = col\n",
            "                        break\n",
            "\n",
            "                if temp_col is None:\n",
            "                    print(f\"Aviso: {os.path.basename(file)} não possui coluna de temperatura máxima\")\n",
            "                    continue\n",
            "\n",
            "                if missing:\n",
            "                    print(f\"Aviso: {os.path.basename(file)} está faltando colunas {missing}\")\n",
            "                    continue\n",
            "\n",
            "                # Criar coluna de data\n",
            "                df['Data'] = pd.to_datetime({\n",
            "                    'year': df['Ano'],\n",
            "                    'month': df['Mes'],\n",
            "                    'day': df['Dia']\n",
            "                })\n",
            "\n",
            "                # Adicionar identificador da estação\n",
            "                station_id = os.path.splitext(os.path.basename(file))[0]\n",
            "                df['station_id'] = station_id\n",
            "\n",
            "                # Renomear coluna de temperatura para um nome mais simples\n",
            "                df = df.rename(columns={temp_col: 'temperatura'})\n",
            "\n",
            "                # Remover linhas com valores nulos na temperatura\n",
            "                df = df.dropna(subset=['temperatura'])\n",
            "\n",
            "                dfs.append(df)\n",
            "                print(f\"Processado com sucesso: {len(df)} registros\")\n",
            "\n",
            "            except Exception as e:\n",
            "                print(f\"Erro ao ler {file}: {e}\")\n",
            "                continue\n",
            "\n",
            "    if not dfs:\n",
            "        raise ValueError(\"Nenhum arquivo válido para processar\")\n",
            "\n",
            "    # Combine all dataframes\n",
            "    print(\"Combining data from all stations...\")\n",
            "    all_df = pd.concat(dfs, ignore_index=True)\n",
            "\n",
            "    # Extrair características temporais\n",
            "    all_df['year'] = all_df['Data'].dt.year\n",
            "    all_df['month'] = all_df['Data'].dt.month\n",
            "    all_df['day'] = all_df['Data'].dt.day\n",
            "    all_df['dayofweek'] = all_df['Data'].dt.dayofweek\n",
            "    all_df['is_weekend'] = all_df['Data'].dt.dayofweek >= 5\n",
            "\n",
            "    # Selecionar apenas as colunas numéricas relevantes para o modelo\n",
            "    feature_cols = ['temperatura', 'year', 'month', 'day', 'dayofweek', 'is_weekend']\n",
            "    all_df = all_df[feature_cols]\n",
            "\n",
            "    # Update x_dim based on actual number of features\n",
            "    global x_dim\n",
            "    x_dim = len(feature_cols)\n",
            "    print(f\"Number of features (x_dim): {x_dim}\")\n",
            "    print(f\"Total number of records: {len(all_df)}\")\n",
            "\n",
            "    return all_df\n",
            "\n",
            "def hyperparameter_tuning(train_X):\n",
            "    def build_model(hp):\n",
            "        # Parâmetros ajustáveis\n",
            "        hp_lstm_dim = hp.Int('lstm_dim', min_value=4, max_value=32, step=4)\n",
            "        hp_z_dim = hp.Int('z_dim', min_value=2, max_value=16, step=2)\n",
            "        hp_learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
            "\n",
            "        model = LSTM_VAE(\n",
            "            time_step,\n",
            "            train_X.shape[2],\n",
            "            hp_lstm_dim,\n",
            "            hp_z_dim\n",
            "        )\n",
            "        model.compile(\n",
            "            optimizer=optimizers.Adam(\n",
            "                learning_rate=hp_learning_rate,\n",
            "                amsgrad=True\n",
            "            )\n",
            "        )\n",
            "        return model\n",
            "\n",
            "    tuner = RandomSearch(\n",
            "        build_model,\n",
            "        objective='loss',\n",
            "        max_trials=10,\n",
            "        executions_per_trial=2,\n",
            "        directory='tuner_results',\n",
            "        project_name='lstm_vae_tuning'\n",
            "    )\n",
            "\n",
            "    print(\"\\nIniciando busca de hiperparâmetros...\")\n",
            "    tuner.search(train_X, epochs=50, batch_size=batch_size)\n",
            "\n",
            "    print(\"\\nMelhores hiperparâmetros encontrados:\")\n",
            "    print(tuner.get_best_hyperparameters(num_trials=1)[0].values)\n",
            "\n",
            "    best_model = tuner.get_best_models(num_models=1)[0]\n",
            "    return best_model\n",
            "\n",
            "def plot_loss_moment(history):\n",
            "    _, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
            "    ax.plot(history['loss'], 'blue', label='Loss', linewidth=1)\n",
            "    ax.plot(history['log_likelihood'], 'red', label='Log likelihood', linewidth=1)\n",
            "    ax.set_title('Loss and log likelihood over epochs')\n",
            "    ax.set_ylabel('Loss and log likelihood')\n",
            "    ax.set_xlabel('Epoch')\n",
            "    ax.legend(loc='upper right')\n",
            "    plt.savefig(image_dir + 'loss_lstm_vae_' + mode + '.png')\n",
            "\n",
            "def plot_log_likelihood(df_log_px):\n",
            "    plt.figure(figsize=(14, 6), dpi=80)\n",
            "    plt.title(\"Log likelihood\")\n",
            "    sns.set_color_codes()\n",
            "    sns.distplot(df_log_px, bins=40, kde=True, rug=True, color='blue')\n",
            "    plt.savefig(image_dir + 'log_likelihood_' + mode + '.png')\n",
            "\n",
            "def save_model(model):\n",
            "    with open(model_dir + 'lstm_vae.json', 'w') as f:\n",
            "        f.write(model.to_json())\n",
            "    model.save_weights(model_dir + 'lstm_vae_ckpt')\n",
            "\n",
            "def load_model():\n",
            "    lstm_vae_obj = {'Encoder': Encoder, 'Decoder': Decoder, 'Sampling': Sampling}\n",
            "    with keras.utils.custom_object_scope(lstm_vae_obj):\n",
            "        with open(model_dir + 'lstm_vae.json', 'r'):\n",
            "            model = keras.models.model_from_json(model_dir + 'lstm_vae.json')\n",
            "        model.load_weights(model_dir + 'lstem_vae_ckpt')\n",
            "    return model\n",
            "\n",
            "def main():\n",
            "    print(\"\\nIniciando processamento...\")\n",
            "    if using_gpu:\n",
            "        print(\"Executando com otimização de GPU\")\n",
            "    else:\n",
            "        print(\"Executando com configuração padrão\")\n",
            "\n",
            "    # Data loading\n",
            "    print(\"Loading and preparing data...\")\n",
            "    all_df = load_and_prepare_data()\n",
            "\n",
            "    # Pre-processing\n",
            "    print(\"Pre-processing data...\")\n",
            "    train_scaled, test_scaled = split_normalize_data(all_df)\n",
            "    print(\"train and test data shape after scaling: \", train_scaled.shape, test_scaled.shape)\n",
            "\n",
            "    train_X = reshape(train_scaled)\n",
            "    test_X = reshape(test_scaled)\n",
            "\n",
            "    if mode == \"train\":\n",
            "        print(\"\\nIniciando otimização de hiperparâmetros...\")\n",
            "        model = hyperparameter_tuning(train_X)\n",
            "\n",
            "        # Treinar com os melhores hiperparâmetros\n",
            "        history = model.fit(\n",
            "            train_X,\n",
            "            epochs=epoch_num,\n",
            "            batch_size=batch_size,\n",
            "            shuffle=False\n",
            "        ).history\n",
            "\n",
            "        model.summary()\n",
            "\n",
            "        # Calcular threshold dinâmico (95º percentil dos erros de treino)\n",
            "        _, _, train_log_px = model.predict(train_X)\n",
            "        train_errors = np.mean(train_log_px, axis=(1,2))\n",
            "        global threshold\n",
            "        threshold = np.percentile(train_errors, 95)  # 95% dos dados normais\n",
            "\n",
            "        # Salvar threshold junto com o modelo\n",
            "        np.save(os.path.join(model_dir, 'threshold.npy'), threshold)\n",
            "        print(f\"Auto Threshold: {threshold:.4f}\")\n",
            "\n",
            "        plot_loss_moment(history)\n",
            "        save_model(model)\n",
            "\n",
            "    elif mode == \"infer\":\n",
            "        model = load_model()\n",
            "        model.compile(optimizer=optimizers.Adam(learning_rate=0.001, epsilon=1e-6, amsgrad=True))\n",
            "\n",
            "        # Carregar threshold salvo\n",
            "        threshold = np.load(os.path.join(model_dir, 'threshold.npy'))\n",
            "    else:\n",
            "        print(\"Unknown mode: \", mode)\n",
            "        exit(1)\n",
            "\n",
            "    _, _, train_log_px = model.predict(train_X, batch_size=1)\n",
            "    train_log_px = train_log_px.reshape(train_log_px.shape[0], train_log_px.shape[2])\n",
            "    df_train_log_px = pd.DataFrame()\n",
            "    df_train_log_px['log_px'] = np.mean(train_log_px, axis=1)\n",
            "    plot_log_likelihood(df_train_log_px)\n",
            "\n",
            "\n",
            "    _, _, test_log_px = model.predict(test_X, batch_size=1)\n",
            "    test_log_px = test_log_px.reshape(test_log_px.shape[0], test_log_px.shape[2])\n",
            "    df_log_px = pd.DataFrame()\n",
            "    df_log_px['log_px'] = np.mean(test_log_px, axis=1)\n",
            "    df_log_px = pd.concat([df_train_log_px, df_log_px])\n",
            "    df_log_px['threshold'] = threshold\n",
            "    df_log_px['anomaly'] = df_log_px['log_px'] > df_log_px['threshold']\n",
            "    df_log_px.index = np.array(all_df)[:, 0]\n",
            "\n",
            "    df_log_px.plot(logy=True, figsize=(16, 9), color=['blue', 'red'])\n",
            "    plt.savefig(image_dir + 'anomaly_lstm_vae_' + mode + '.png')\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "------------------\n",
            "\n",
            "----- stderr -----\n",
            "WARNING:tensorflow:From <ipython-input-1-7d748eb7f54a>:33: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "----- stdout -----\n",
            "\n",
            "Verificando configuração GPU:\n",
            "TensorFlow versão: 2.18.0\n",
            "CUDA disponível: True\n",
            "GPU disponível para TensorFlow: False\n",
            "\n",
            "Nenhuma GPU encontrada. Usando CPU.\n",
            "----- stdout -----\n",
            "\n",
            "Iniciando processamento...\n",
            "Executando com configuração padrão\n",
            "Loading and preparing data...\n",
            "Loading and preparing data...\n",
            "\n",
            "WARNING: No Excel files found in C:\\F_analises\\INMET\\Convencionais processadas temperaturas\n",
            "Please make sure the Excel files are in the correct directory.\n",
            "----- stderr -----\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n",
            "------------------\n",
            "\n",
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n",
            "\n",
            "\n",
            "\n",
            "Error executing notebook\n"
          ]
        }
      ]
    }
  ]
}